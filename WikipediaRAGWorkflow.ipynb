{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a9ed9f-32ef-48f2-9eaa-b13914b25a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.workflow import Event\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "class IngestEvent(Event):\n",
    "    index: VectorStoreIndex\n",
    "    \n",
    "class RetrieverEvent(Event):\n",
    "    nodes: list[NodeWithScore]\n",
    "\n",
    "class RerankEvent(Event):\n",
    "    nodes: list[NodeWithScore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1fdeb55-8083-40a5-8c0c-28001a5a817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response_synthesizers import CompactAndRefine\n",
    "from llama_index.core.postprocessor.llm_rerank import LLMRerank\n",
    "from llama_index.core.workflow import Context, Workflow, StartEvent, StopEvent, step\n",
    "\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.readers.wikipedia import WikipediaReader\n",
    "\n",
    "import wikipedia\n",
    "from wikipedia import PageError\n",
    "\n",
    "class RAGWorkflow(Workflow):\n",
    "    @step\n",
    "    async def ingest(self, ctx: Context, ev: StartEvent) -> IngestEvent | None:\n",
    "        query = ev.get(\"query\", None)\n",
    "        if query is None:\n",
    "            return None\n",
    "\n",
    "        await ctx.set(\"query\", query)\n",
    "\n",
    "        pages = wikipedia.search(query, results=10)\n",
    "        if not pages:\n",
    "            return None\n",
    "\n",
    "        wiki_loader = WikipediaReader()\n",
    "        documents = []\n",
    "        for page in pages:\n",
    "            try:\n",
    "                doc = wiki_loader.load_data([page], lang_prefix=\"en\")\n",
    "                documents.extend(doc)\n",
    "            except PageError:\n",
    "                print(f\"Skipping “{page}” (PageError).\")\n",
    "\n",
    "        index = VectorStoreIndex.from_documents(\n",
    "            documents=documents,\n",
    "            embed_model=OpenAIEmbedding(model_name=\"text-embedding-3-small\"),\n",
    "        )\n",
    "\n",
    "        return IngestEvent(index=index)\n",
    "\n",
    "    @step\n",
    "    async def retrieve(self, ctx: Context, ev: IngestEvent) -> RetrieverEvent | None:\n",
    "        index = ev.index\n",
    "        query = await ctx.get(\"query\", None)\n",
    "        if (query is None) or (index is None):\n",
    "            return None\n",
    "\n",
    "        retriever = index.as_retriever(similarity_top_k=5)\n",
    "        nodes = await retriever.aretrieve(query)\n",
    "        print(f\"Retrieved {len(nodes)} nodes.\")\n",
    "        return RetrieverEvent(nodes=nodes)\n",
    "\n",
    "    @step\n",
    "    async def rerank(self, ctx: Context, ev: RetrieverEvent) -> RerankEvent:\n",
    "        ranker = LLMRerank(\n",
    "            choice_batch_size=5,\n",
    "            top_n=3,\n",
    "            llm=OpenAI(model=\"gpt-4o\"),\n",
    "        )\n",
    "        query = await ctx.get(\"query\", default=None)\n",
    "        new_nodes = ranker.postprocess_nodes(ev.nodes, query_str=query)\n",
    "        print(f\"Reranked nodes to {len(new_nodes)}\")\n",
    "        return RerankEvent(nodes=new_nodes)\n",
    "\n",
    "    @step\n",
    "    async def synthesize(self, ctx: Context, ev: RerankEvent) -> StopEvent:\n",
    "        llm = OpenAI(model=\"gpt-4o\")\n",
    "        summarizer = CompactAndRefine(llm=llm, streaming=True, verbose=True)\n",
    "        query = await ctx.get(\"query\", default=None)\n",
    "        \n",
    "        response = await summarizer.asynthesize(query, nodes=ev.nodes)\n",
    "        return StopEvent(result=response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2367828-d00e-4f0c-918f-a2a233f70150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step ingest\n",
      "Skipping “GPT-4” (PageError).\n",
      "Skipping “GPT-3” (PageError).\n",
      "Step ingest produced event IngestEvent\n",
      "Running step retrieve\n",
      "Retrieved 5 nodes.\n",
      "Step retrieve produced event RetrieverEvent\n",
      "Running step rerank\n",
      "Reranked nodes to 3\n",
      "Step rerank produced event RerankEvent\n",
      "Running step synthesize\n",
      "Step synthesize produced event StopEvent\n",
      "A transformer is a deep learning architecture that utilizes a multi-head attention mechanism to process and contextualize text data. It converts text into numerical representations called tokens, which are then transformed into vectors using a word embedding table. The architecture allows for parallel processing of tokens, enhancing the importance of key tokens while diminishing less important ones. This design eliminates the need for recurrent units, making transformers more efficient in training compared to earlier recurrent neural networks like LSTMs.\n",
      "\n",
      "In large language models (LLMs), transformers are used to handle vast amounts of text data for natural language processing tasks. They form the backbone of models such as GPTs and BERT, which are pre-trained on extensive datasets to perform tasks like language generation, translation, and summarization. Transformers have become the standard architecture for developing LLMs due to their ability to efficiently process and generate human-like text."
     ]
    }
   ],
   "source": [
    "w = RAGWorkflow(timeout=60, verbose=True)\n",
    "\n",
    "result = await w.run(query=\"What is a transformer, and how is it used in large language models?\")\n",
    "async for chunk in result.async_response_gen():\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8895bee1-f971-4c52-af12-7efa8cde5369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia_rag_workflow.html\n"
     ]
    }
   ],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(RAGWorkflow, filename=\"wikipedia_rag_workflow.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb552057-d0af-4ac5-94d5-a9d48665e1b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
